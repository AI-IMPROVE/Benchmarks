{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d4a2b47d029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import argparse\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#if gpus:\n",
    "#    try:\n",
    "#        for gpu in gpus:\n",
    "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#    except RuntimeError as e:\n",
    "#        print(e)\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.dirname(os.path.realpath(__file__))\n",
    "lib_path = os.path.abspath(os.path.join(file_path, '..', '..', 'common'))\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "\n",
    "psr = argparse.ArgumentParser(description='input csv file')\n",
    "psr.add_argument('--in_train',  default='in_train')\n",
    "psr.add_argument('--in_vali',  default='in_vali')\n",
    "psr.add_argument('--ep',  type=int, default=400)\n",
    "args=vars(psr.parse_args())\n",
    "print(args)\n",
    "\n",
    "EPOCH = args['ep']\n",
    "BATCH = 32\n",
    "BATCH = BATCH * strategy.num_replicas_in_sync\n",
    "\n",
    "data_path_train = args['in_train']\n",
    "data_path_vali = args['in_vali']\n",
    "\n",
    "DR    = 0.1      # Dropout rate                  \n",
    "\n",
    "### define r2 for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "## Implement a Transformer block as a layer\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "## Implement embedding layer\n",
    "## Two seperate embedding layers, one for tokens, one for token index (positions).\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and prepare dataset\n",
    "\n",
    "vocab_size = 40000  # \n",
    "maxlen = 250  # \n",
    "\n",
    "\n",
    "data_train = pd.read_csv(data_path_train)\n",
    "data_vali = pd.read_csv(data_path_vali)\n",
    "\n",
    "data_train.head()\n",
    "\n",
    "# Dataset has type and smiles as the two fields\n",
    "\n",
    "y_train = data_train[\"type\"].values.reshape(-1, 1) * 1.0\n",
    "y_val = data_vali[\"type\"].values.reshape(-1, 1) * 1.0\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data_train[\"smiles\"])\n",
    "\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    # Turns text into into padded sequences.\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(text_sequences, maxlen=maxlen)\n",
    "\n",
    "x_train = prep_text(data_train[\"smiles\"], tokenizer, maxlen)\n",
    "x_val = prep_text(data_vali[\"smiles\"], tokenizer, maxlen)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# As x,y values are popped off the dataset, each needs to be in the form\n",
    "# of a one dimensional np.array with lengths 1 and 250 (maxlen=250)\n",
    "\n",
    "# See if this work for inmemory distributed data\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensors(([1.], [1.])) # are the parens correct?\n",
    "# map_func, num_parallel_calls=None, deterministic=None, name=None\n",
    "\n",
    "x_train = tf.data.Dataset.from_tensors(x_train)\n",
    "y_train = tf.data.Dataset.from_tensors(y_train)\n",
    "\n",
    "x_train = x_train.map(lambda x : x.numpy(), num_parallel_calls=40)\n",
    "y_train = y_train.map(lambda y : y.numpy(), num_parallel_calls=40)\n",
    "\n",
    "x_train = tf.distribure.DistributedDataset(x_train)\n",
    "y_train = tf.distribute.DistributedDataset(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create regression/classifier model using N transformer layers\n",
    "\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 16  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "with strategy.scope():\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    #x = layers.GlobalAveragePooling1D()(x)  --- the original model used this but the accuracy was much lower\n",
    "\n",
    "    x = layers.Reshape((1,32000), input_shape=(250,128,))(x)  # reshaping increases parameters but improves accuracy a lot\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(1, activation='relu')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    ## Train and Evaluate\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(lr=0.00001),\n",
    "                  metrics=['mae',r2])\n",
    "\n",
    "# set up a bunch of callbacks to do work during model training..\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='smile_regress.autosave.model.h5', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "csv_logger = CSVLogger('smile_regress.training.log')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=20, verbose=1, mode='auto', epsilon=0.0001, cooldown=3, min_lr=0.000000001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, verbose=1, mode='auto')\n",
    "\n",
    "print (\"fitting on model with train shape {} and validation shape {}\".format(\n",
    "    x_train.shape, x_val.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH,\n",
    "                    epochs=EPOCH,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks = [checkpointer,csv_logger, reduce_lr, early_stop])\n",
    "\n",
    "model.load_weights('smile_regress.autosave.model.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
